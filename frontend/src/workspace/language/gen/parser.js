// This file was generated by lezer-generator. You probably shouldn't edit it.
import {LRParser} from "@lezer/lr"
const spec_ParticipantName = {__proto__:null,title:8, participant:12, label:18}
export const parser = LRParser.deserialize({
  version: 14,
  states: "%WQYQPOOOeOPO'#CmOjQPO'#CpOOQO'#Cg'#CgQoQPOOOwQPO'#CrO!VQPO'#CqOOQO'#Ch'#ChQ!bQPOOQoQPOOO!gOQO,59XO!lQPO,59[OOQO-E6e-E6eOOQO'#Cc'#CcO!qQPO,59^O!yOPO'#CzOOQO'#Ci'#CiO#OQPO,59]OOQO-E6f-E6fO#ZOQO1G.sOOQO1G.v1G.vO#`OPO'#CdOOQO1G.x1G.xO#eQPO1G.xO#jOSO,59fOOQO-E6g-E6gOOQO7+$_7+$_O#oOPO,59OOOQO7+$d7+$dO#tOQO1G/QOOQO1G.j1G.jOOQO7+$l7+$l",
  stateData: "#y~O`OSPOS~ORTOSPOUQO~ObYO~ObZO~ORTOUQO~Og]Oh]Oi]Oj]O~OX_OReX^eX~ORTO~OTcO~ORdO~ORfOkeO~ObhO~OX_ORea^ea~OcjO~OlkO~ORlO~OYmO~OmnO~OcoO~O",
  goto: "!uoPPPPPPPpsPPv}!VPPP!]PP!`!e!kPPPPPPP!qR^TRg^SSOXR[SUWOSXRbWQaURiaRXOVROSXXVOSWXXUOSWXT`Ua",
  nodeNames: "âš  Comment Program ParticipantName title TitleText participant Arrow Delay label LabelText",
  maxTerm: 30,
  skippedNodes: [0,1],
  repeatNodeCount: 3,
  tokenData: "(yVRdOX!aXY!nYZ#iZp!apq!nqs!ast#ytx!axy$Yyz$iz}!a}!O$x!O!Q!a!Q!['b![!]'w!]!c!a!c!}(W!}#T!a#T#o(W#o~!aU!hQYSTQOY!aZ~!aV!wUYSTQ`POX!aXY!nYZ#ZZp!apq!nq~!aP#`R`PXY#ZYZ#Zpq#ZR#pRcQ`PXY#ZYZ#Zpq#ZV$SQPPYSTQOY#yZ~#yV$cQkPYSTQOY!aZ~!aV$rQmPYSTQOY!aZ~!aV%PUYSTQOY!aZ}!a}!O%c!O!`!a!`!a&l!a~!aV%jSYSTQOY!aZ!`!a!`!a%v!a~!aV&PSiPYSTQOY!aZ!`!a!`!a&]!a~!aV&fQjPYSTQOY!aZ~!aV&uSgPYSTQOY!aZ!`!a!`!a'R!a~!aV'[QhPYSTQOY!aZ~!aV'kSYSTQlPOY!aZ!Q!a!Q!['b![~!aV(QQbPYSTQOY!aZ~!aV(aWYSRPTQOY!aZ!Q!a!Q![(W![!c!a!c!}(W!}#T!a#T#o(W#o~!a",
  tokenizers: [0, 1, 2],
  topRules: {"Program":[0,2]},
  specialized: [{term: 3, get: value => spec_ParticipantName[value] || -1}],
  tokenPrec: 0
})
